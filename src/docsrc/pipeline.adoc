= Pipeline

[sidebar]
{code-loc}/pipeline.sh[/pipeline.sh]

Baraa's project uses stored procedures for orchestration, but no clear flow to kick
them off other than manually connecting to the db and running EXEC on the procedures.
I wanted to run the scripts in a clean and automated way. After briefly looking at Airflow,
I decided to write the ETL pipeline with Bash (`pipeline.sh`), since I started testing
things with `psql` on the command line and am not ready to dive into learning the Airflow
syntax just yet. Simple. Each bash function does kick off SQL commands, though, some as
temporary procedures (DO blocks).

The `pipeline.sh` script calls the SQL scripts with psql, with output to the command line
and a log in `/tmp/sql-data-warehouse/`.

To run the full pipeline:

```
sudo -u postgres ./pipeline.sh full
```

There are other options besides 'full', if you want to run isolated stages of the pipeline.
Here are the stages each option runs:

[source,bash]
----
include::../../pipeline.sh[tag=options]
----


